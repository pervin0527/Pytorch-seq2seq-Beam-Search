{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pervinco/miniconda3/envs/DL/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import functional as F\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "data_dir = '/home/pervinco/Desktop/en-fr/data'\n",
    "\n",
    "total_data_dir = f'{data_dir}/eng-fra.txt'\n",
    "train_data_dir = f'{data_dir}/train.txt'\n",
    "valid_data_dir = f'{data_dir}/valid.txt'\n",
    "test_data_dir = f'{data_dir}/test.txt'\n",
    "src_lang, trg_lang = 'eng', 'fra'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(total_path, train_path, valid_path, test_path, train_ratio=0.8, valid_ratio=0.1, test_ratio=0.1):\n",
    "    with open(total_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    random.shuffle(lines)\n",
    "\n",
    "    total_size = len(lines)\n",
    "    train_size = int(total_size * train_ratio)\n",
    "    valid_size = int(total_size * valid_ratio)\n",
    "    test_size = total_size - train_size - valid_size\n",
    "\n",
    "    train_data = lines[:train_size]\n",
    "    valid_data = lines[train_size:train_size + valid_size]\n",
    "    test_data = lines[train_size + valid_size:]\n",
    "\n",
    "    with open(train_path, 'w', encoding='utf-8') as train_file:\n",
    "        train_file.writelines(train_data)\n",
    "\n",
    "    with open(valid_path, 'w', encoding='utf-8') as valid_file:\n",
    "        valid_file.writelines(valid_data)\n",
    "\n",
    "    with open(test_path, 'w', encoding='utf-8') as test_file:\n",
    "        test_file.writelines(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(train_data_dir) or not os.path.exists(valid_data_dir):\n",
    "    split_data(total_data_dir, train_data_dir, valid_data_dir, test_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "유니코드 문자열을 아스키 문자열로 변환. 이 과정을 통해 텍스트 데이터의 일관성을 높인다.\n",
    "\"\"\"\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\"\"\"\n",
    "텍스트를 소문자로 변환하고, 불필요한 공백이나 문자가 아닌 문자를 제거해 모델의 학습 데이터 품질을 향상시킨다.\n",
    "\"\"\"\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_build_vocab(lang, pairs):\n",
    "    if lang == 'eng':\n",
    "        tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "    elif lang == 'fra':\n",
    "        tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported language: {lang}\")\n",
    "\n",
    "    vocab = build_vocab_from_iterator(tokenizer(pair[0]) if lang == 'eng' else tokenizer(pair[1]) for pair in pairs)\n",
    "    vocab.insert_token('<pad>', PAD_TOKEN)\n",
    "    vocab.insert_token('<sos>', SOS_TOKEN)\n",
    "    vocab.insert_token('<eos>', EOS_TOKEN)\n",
    "    vocab.insert_token('<unk>', UNK_TOKEN)\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "    return vocab, tokenizer\n",
    "\n",
    "def build_vocab(data_dir, src_lang='eng', trg_lang='fra', save_dir=None):\n",
    "    lines = open(data_dir, encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    pairs = filterPairs(pairs)\n",
    "\n",
    "    src_vocab, src_tokenizer = tokenize_and_build_vocab(src_lang, pairs)\n",
    "    trg_vocab, trg_tokenizer = tokenize_and_build_vocab(trg_lang, pairs)\n",
    "\n",
    "    if save_dir:\n",
    "        torch.save(src_vocab, os.path.join(save_dir, f'src_vocab_{src_lang}.pth'))\n",
    "        torch.save(trg_vocab, os.path.join(save_dir, f'trg_vocab_{trg_lang}.pth'))\n",
    "\n",
    "    return src_vocab, src_tokenizer, trg_vocab, trg_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'{data_dir}/src_vocab_{src_lang}.pth') and not os.path.exists(f'{data_dir}/trg_vocab_{trg_lang}.pth'):\n",
    "    src_vocab, src_tokenizer, trg_vocab, trg_tokenizer = build_vocab(total_data_dir, src_lang, trg_lang, data_dir)\n",
    "else:\n",
    "    src_vocab = torch.load(f'{data_dir}/src_vocab_{src_lang}.pth')\n",
    "    trg_vocab = torch.load(f'{data_dir}/trg_vocab_{trg_lang}.pth')\n",
    "\n",
    "    if src_lang == 'eng':\n",
    "        src_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "        trg_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')\n",
    "    elif src_lang == 'fra':\n",
    "        src_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')\n",
    "        trg_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data_dir, src_vocab, trg_vocab, src_tokenizer, trg_tokenizer, src_lang='eng', trg_lang='fra', max_length=50):\n",
    "        self.src_lang = src_lang\n",
    "        self.trg_lang = trg_lang\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "        \n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "        \n",
    "        lines = open(data_dir, encoding='utf-8').read().strip().split('\\n')\n",
    "        self.pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "        self.pairs = filterPairs(self.pairs)\n",
    "        \n",
    "        if src_lang == 'fra':\n",
    "            self.pairs = [list(reversed(p)) for p in self.pairs]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_sentence, output_sentence = self.pairs[idx]\n",
    "        \n",
    "        input_tokens = self.src_tokenizer(input_sentence)\n",
    "        output_tokens = self.trg_tokenizer(output_sentence)\n",
    "        \n",
    "        input_tensor = [self.src_vocab['<sos>']] + [self.src_vocab[token] if token in self.src_vocab else self.src_vocab['<unk>'] for token in input_tokens] + [self.src_vocab['<eos>']]\n",
    "        output_tensor = [self.trg_vocab['<sos>']] + [self.trg_vocab[token] if token in self.trg_vocab else self.trg_vocab['<unk>'] for token in output_tokens] + [self.trg_vocab['<eos>']]\n",
    "        \n",
    "        return torch.tensor(input_tensor, dtype=torch.long), torch.tensor(output_tensor, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   1,    5,   23,  128,   65,   28,  226,    7, 3205,    7,   57,    4,\n",
      "           2])\n",
      "tensor([  1,   5,  31,  33, 569,   6,  65, 446,  90, 327,   4,   2])\n"
     ]
    }
   ],
   "source": [
    "dataset = TranslationDataset(train_data_dir, src_vocab, trg_vocab, src_tokenizer, trg_tokenizer, src_lang, trg_lang, MAX_LENGTH)\n",
    "src, trg = dataset[0]\n",
    "print(src)\n",
    "print(trg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
